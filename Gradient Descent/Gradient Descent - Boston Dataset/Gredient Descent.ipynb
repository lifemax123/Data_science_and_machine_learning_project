{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcdf3d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost 281.7160204856784\n",
      "Iteration 100: Cost 47.56313535421048\n",
      "Iteration 200: Cost 38.085415801109896\n",
      "Iteration 300: Cost 32.8674570311854\n",
      "Iteration 400: Cost 29.749907610193883\n",
      "Iteration 500: Cost 27.711041365905288\n",
      "Iteration 600: Cost 26.24494731414347\n",
      "Iteration 700: Cost 25.099170979765034\n",
      "Iteration 800: Cost 24.14602364120028\n",
      "Iteration 900: Cost 23.319423500563246\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the data\n",
    "train_data = np.genfromtxt(\"C:/Users/lifem/Downloads/0000000000002417_training_boston_x_y_train.csv\", delimiter=',')\n",
    "test_data = np.genfromtxt(\"C:/Users/lifem/Downloads/0000000000002417_test_boston_x_test.csv\", delimiter=',')\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "X_train = train_data[:, :-1]\n",
    "Y_train = train_data[:, -1]\n",
    "X_test = test_data\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize Parameters\n",
    "def initialize_parameters(n):\n",
    "    W = np.zeros(n)\n",
    "    b = 0\n",
    "    return W, b\n",
    "\n",
    "# Gradient Descent\n",
    "def compute_cost(X, Y, W, b):\n",
    "    m = X.shape[0]\n",
    "    predictions = X.dot(W) + b\n",
    "    cost = (1 / (2 * m)) * np.sum((predictions - Y) ** 2)\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, Y, W, b, learning_rate, num_iterations):\n",
    "    m = X.shape[0]\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        predictions = X.dot(W) + b\n",
    "        errors = predictions - Y\n",
    "\n",
    "        dW = (1 / m) * X.T.dot(errors)\n",
    "        db = (1 / m) * np.sum(errors)\n",
    "\n",
    "        W -= learning_rate * dW\n",
    "        b -= learning_rate * db\n",
    "\n",
    "        cost = compute_cost(X, Y, W, b)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Cost {cost}\")\n",
    "\n",
    "    return W, b, cost_history\n",
    "def predict(X, W, b):\n",
    "    return X.dot(W) + b\n",
    "\n",
    "# Number of features\n",
    "n_features = X_train_scaled.shape[1]\n",
    "\n",
    "# Initialize weights and bias\n",
    "W, b = initialize_parameters(n_features)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# Run gradient descent\n",
    "W, b, cost_history = gradient_descent(X_train_scaled, Y_train, W, b, learning_rate, num_iterations)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = predict(X_test_scaled, W, b)\n",
    "\n",
    "# Save the predictions\n",
    "np.savetxt('predictions2.csv', predictions, delimiter=',', fmt='%.5f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e1efd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
